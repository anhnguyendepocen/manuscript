\newpage\section*{Responses to Referee}\vspace{1.0cm}
%-------------------------------------------------------------------------------
Thank you very much for your constructive remarks that helped improve the manuscript. I hope that I was able to address each of the comments to your satisfaction. Please note that nearly all the new material is available in the Appendix as a 10 page limit for the main manuscript is strictly enforced by the editorial office. In addition to the changes due to your remarks, I have also improved the setup of my investigation regarding the impact of the interpolation grid on estimation performance by setting this up as a Monte Carlo exercise as well. For details, please see the text. All other aspects of the paper remain mostly unchanged and slightly different results are only due to a refactoring of the bootstrap samples that affected the random draws.
%-------------------------------------------------------------------------------
\begin{boenumerate}
%-------------------------------------------------------------------------------
%	Comment 1
%-------------------------------------------------------------------------------
\item \textit{First, the KW paper can be thought of as consisting of two parts: First, an analysis of the accuracy of the approximate solution in terms of the value functions and policy functions. Second, an analysis of how using approximate solutions for DCDP models affects parameter estimates when those models are estimated. The part of the present paper that deals with accuracy of the approximate solution seems fine, but the part of the paper that deals with estimation (starting on the middle of page 8) is very confusing.}\vspace{0.5cm}

Thank you for this comment.\vspace{0.50cm}
%-------------------------------------------------------------------------------
%	Comment 2
%-------------------------------------------------------------------------------
\item \textit{The main problem is that the author never writes out the choice probabilities or the likelihood function for the model, and he doesn't explain how they are simulated. According to KW, they use 200 draws and a kernel smoothing algorithm to smooth the likelihood. But the author doesn't discuss what he does here. This is critical, because his main finding is that there are local maxima problems in the simulated likelihood. But this may well be because of how the tuning parameter is set in the kernel smoothing algorithm. This needs to be discussed. One proposal that has been made by KW in other work is to start with a large bandwidth and then make it smaller as you approach the optimum (or, alternatively, as the author states here, to increase the number of draws as one approaches the optimum).}\vspace{0.5cm}

\cite{Keane.1994} use the kernel smoothing function as described in \citet{McFadden.1989} and note in footnote 23 that they use a window parameter of 500. I follow their lead unless otherwise noted which is now stated in Appendix \ref{Computational Details}. Nevertheless, I also investigated the impact of the choice of the window parameter on estimation performance. Using the estimates from a static model as the starting values, I ran the estimator with alternative values of the window parameter. The results are available in Table \ref{Smoothing Schemes}. I have also experimented with an adaptive strategy where I successively reduced the window parameter by one-fourth each time the optimizer terminates. Both approaches lead me to the conclusion that  varying the window parameter allows to reduce the RMSE slightly, however it is no substitute for a finer interpolation grid to ensure the reliability of results. This summary also lines up with the experiences in my own applied work.
%-------------------------------------------------------------------------------
%	Comment 3
%-------------------------------------------------------------------------------
\item \textit{Another limitation of the paper is that it misses a great opportunity to truly update the KW results. Of course computers are vastly faster now than they were in 1994. I assume this means that it should be possible to approximate DP solutions much more accurately than KW did back in 1994. For example, KW presumably report computation times for their exact and approximate solutions. How much faster are these times now? Conversely, if one were willing to solve the problem in the same time as KW, how much more accurate could you make the solution today (by using more draws and/or state points)? Or, one could ask how much bigger of a problem could be solved today to the same accuracy and computation time. One easy way to look at this might be to expand the state space by increasing the time horizon (T).}\vspace{0.5cm}

I very much agree that it is useful to assess the contribution of \citet{Keane.1994} in its historical context. Thus, I added the proposed extension to Appendix \ref{Computational Details}.
%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------s
\end{boenumerate}
