\newpage\section*{Responses to Referee}\vspace{1.0cm}
%-------------------------------------------------------------------------------
Thank you very much for your constructive remarks. I hope that I was able to address each of the comments to your satisfaction. Please note that nearly all the new material is only available in the Appendix as a 10 page limit for the main paper is strictly enforced by the editorial office. In addition to the changes due to your remarks, I have also improved the setup of my investigation regarding the impact of the interpolation grid on estimation performance by setting this up as a Monte Carlo exercise as well. For details, please see the text. All other aspects of the paper remain mostly unchanged. The slightly different results are only due to a refactoring of the Monte Carlo samples that affected the random subsamples.
%-------------------------------------------------------------------------------
\begin{boenumerate}
%-------------------------------------------------------------------------------
%	Comment 1
%-------------------------------------------------------------------------------
\item \textit{First, the KW paper can be thought of as consisting of two parts: First, an analysis of the accuracy of the approximate solution in terms of the value functions and policy functions. Second, an analysis of how using approximate solutions for DCDP models affects parameter estimates when those models are estimated. The part of the present paper that deals with accuracy of the approximate solution seems fine, but the part of the paper that deals with estimation (starting on the middle of page 8) is very confusing.}\vspace{0.5cm}

Thank you very much for pointing this out. I have rewritten the relevant parts of the paper based on the comments of numerous research assistants. I hope it is clear now.
%-------------------------------------------------------------------------------
%	Comment 2
%-------------------------------------------------------------------------------
\item \textit{The main problem is that the author never writes out the choice probabilities or the likelihood function for the model, and he doesn't explain how they are simulated. According to KW, they use 200 draws and a kernel smoothing algorithm to smooth the likelihood. But the author doesn't discuss what he does here. This is critical, because his main finding is that there are local maxima problems in the simulated likelihood. But this may well be because of how the tuning parameter is set in the kernel smoothing algorithm. This needs to be discussed. One proposal that has been made by KW in other work is to start with a large bandwidth and then make it smaller as you approach the optimum (or, alternatively, as the author states here, to increase the number of draws as one approaches the optimum).}\vspace{0.5cm}

\cite{Keane.1994} use the kernel smoothing function as described in \citet{McFadden.1989} with a window parameter of 500 (footnote 23). I follow their example which is now clearly stated in Appendix \ref{Computational Details}. To further address your concerns, I also investigated the impact of the choice of the window parameter on estimation performance in more detail. I use the same Monte Carlo setup as in the analysis of alternative interpolation schemes in Section \ref{Approximation}. However, I now fix the number of interpolation points at 200 and instead vary the value of the window parameter. I study two alternative approaches.

\paragraph{Fixed Smoothing Parameters} I simply start the estimations with different window parameters $\tau$ which remain fixed throughout. Table \ref{Fixed Scale Parameters} shows the results. The RMSE remains virtually unchanged at 0.1.

\input{material/table_smoothing.tex}

\paragraph{Adaptive Smoothing Parameters} I also run an estimation where I iteratively reduce the value of $\tau$ as suggested in \citet{Keane.2003}.\footnote{See \citet{Bruins.2015} for the detailed asymptotic and computational analysis of the ideas proposed in \citet{Keane.2003}.} I start the estimation with the baseline value of 500 and each time the optimizer terminates, I reduce its value by one quarter. I repeat this process until there is no further reduction in the RMSE. This allows to reduce the average RMSE from 0.1 to about 0.07.\newline

To sum up, experimentation with the smoothing parameter does allow to slightly reduce the RMSE. However, it is not a substitute for a finer interpolation grid to ensure the reliability of results. This also lines up with the experience in my applied work.\newline
%-------------------------------------------------------------------------------
%	Comment 3
%-------------------------------------------------------------------------------
\item \textit{Another limitation of the paper is that it misses a great opportunity to truly update the KW results. Of course computers are vastly faster now than they were in 1994. I assume this means that it should be possible to approximate DP solutions much more accurately than KW did back in 1994. For example, KW presumably report computation times for their exact and approximate solutions. How much faster are these times now? Conversely, if one were willing to solve the problem in the same time as KW, how much more accurate could you make the solution today (by using more draws and/or state points)? Or, one could ask how much bigger of a problem could be solved today to the same accuracy and computation time. One easy way to look at this might be to expand the state space by increasing the time horizon (T).}\vspace{0.5cm}

I very much agree that it is useful to assess the contribution of \citet{Keane.1994} in its historical context. Thus, I added the proposed extension to Appendix \ref{Appendix: Historical Perspective}.
%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------s
\end{boenumerate}
